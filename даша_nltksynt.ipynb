{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Написание правил для дат (типа «Июль 1993»и т.п.)\n",
        "В парсере есть только количественные конструкции (пять котов),\n",
        "необходимо добавить правила для\n",
        "групп типа пятое июля 1993 года, пятого июля 1993 года и т.д. в падежных формах.\n"
      ],
      "metadata": {
        "id": "B19FBr4bzrDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "id": "-VNXA1lTz7px",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad4841a-604e-4f17-c8d8-feac522a74fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=b8c1244b078a0767c620e44ba93ef9900a073313b75587a83e6357f38429f663\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import *\n",
        "import nltk\n",
        "nltk.download('book_grammars')\n",
        "import pymorphy2 as pm\n",
        "import codecs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGArClQLw3El",
        "outputId": "5467176e-4346-430c-d5a5-77a5b5fbc77f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package book_grammars to /root/nltk_data...\n",
            "[nltk_data]   Unzipping grammars/book_grammars.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## загружаем PyMorphy2\n",
        "m = pm.MorphAnalyzer()\n",
        "## открываем (создаем)файл с грамматикой, куда будут записываться правила\n",
        "f = codecs.open(\"test.fcfg\", mode= \"w\", encoding = \"utf-8\")\n",
        "rules = codecs.open(\"/content/rules_dasha.txt\", mode= \"r\", encoding = \"utf-8\")\n",
        "## записываем правила, которые вручную делаем (некоторые на основе правил из АОТ)\n",
        "for rule in rules:\n",
        "\tf.writelines(rules)\n",
        "\n",
        "rules.close()\n",
        "f.close()"
      ],
      "metadata": {
        "id": "mg0fr6vKw8Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## функция, которая переводит нужную нам информацию из пайморфи в вид, читаемый парсером NLTK\n",
        "## принимает (токенизированное) словосочетание на входе, записывает правила (lexical productions) в тот же файл с грамматикой"
      ],
      "metadata": {
        "id": "FrQjTlP3xBSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pm2fcfg (phrase): ## phrase - это словосочетание, которое мы разбираем\n",
        "    f = codecs.open(\"test.fcfg\", mode= \"a\", encoding = \"utf-8\")\n",
        "    for x in phrase:\n",
        "        a = m.parse(x) ## a - список возможных вариантов морфологического разбора слова, предлагаемых пайморфи\n",
        "\t\t## от части речи зависит, какие признаки отправляются в грамматику, осюда условия\n",
        "        for y in a:\n",
        "            if (y.tag.POS == \"NOUN\") or (y.tag.POS == \"ADJF\") or (y.tag.POS == \"PRTF\"):\n",
        "                strk = str(y.tag.POS) + \"[C=\" + str(y.tag.case) + \", G=\" + str(y.tag.gender) + \", NUM=\" + str(y.tag.number) + \", PER=3\" + \", NF=u'\" + str(y.normal_form) + \"'] -> '\" + str(y.word) + \"'\\n\"\n",
        "                f.writelines(strk)\n",
        "            elif (y.tag.POS == \"ADJS\") or (y.tag.POS == \"PRTS\"):\n",
        "                strk = str(y.tag.POS) + \"[G=\" + str(y.tag.gender) + \", NUM=\" + str(y.tag.number) + \", NF=u'\" + str(y.normal_form) + \"'] -> '\" + str(y.word) + \"'\\n\"\n",
        "                f.writelines(strk)\n",
        "            elif (y.tag.POS == \"NUMR\"):\n",
        "                strk = str(y.tag.POS) + \"[C=\" + str(y.tag.case) + \", NF=u'\" + str(y.normal_form) + \"'] -> '\" + str(y.word) + \"'\\n\"\n",
        "                f.writelines(strk)\n",
        "            elif (y.tag.POS == \"ADVB\") or (y.tag.POS == \"GRND\") or (y.tag.POS == \"COMP\") or (y.tag.POS == \"PRED\") or (y.tag.POS == \"PRCL\") or (y.tag.POS == \"INTJ\"):\n",
        "                strk = str(y.tag.POS) + \"[NF=u'\" + str(y.normal_form) + \"'] -> '\" + str(y.word) + \"'\\n\"\n",
        "                f.writelines(strk)\n",
        "            elif (y.tag.POS == \"PREP\") or (y.tag.POS == \"CONJ\"):\n",
        "                strk = str(y.tag.POS) + \"[NF=u'\" + str(y.normal_form) + \"'] -> '\" + str(y.word) + \"'\\n\"\n",
        "                f.writelines(strk)\n",
        "                break\n",
        "            elif (y.tag.POS == \"NPRO\") & (y.normal_form != \"это\")& (y.normal_form != \"нечего\"):\n",
        "                if ((y.tag.person[0] == \"3\") & (y.tag.number == \"sing\")):\n",
        "                    strk = str(y.tag.POS) + \"[C=\" + str(y.tag.case) + \", G=\" + str(y.tag.gender) + \", NUM=\" + str(y.tag.number) + \", PER=\" + str(y.tag.person)[0] + \", NF=u'\" + str(y.normal_form) + \"'] -> '\" + str(y.word) + \"'\\n\"\n",
        "                else:\n",
        "                    strk = str(y.tag.POS) + \"[C=\" + str(y.tag.case) + \", NUM=\" + str(y.tag.number) + \", PER=\" + str(y.tag.person)[0] + \", NF=u'\" + str(y.normal_form) + \"'] -> '\" + str(y.word) + \"'\\n\"\n",
        "                f.writelines(strk)\n",
        "            elif (y.tag.POS == \"VERB\")  or (y.tag.POS == \"INFN\"):\n",
        "                if (y.tag.tense == \"past\"):\n",
        "                    strk = str(y.tag.POS) + \"[TR=\" + str(y.tag.transitivity) + \", TENSE=\" + str(y.tag.tense) + \", G=\" + str(y.tag.gender) + \", NUM=\" + str(y.tag.number) + \", PER=\" + \"0\" + \", NF=u'\" + str(y.normal_form) + \"'] -> '\" + str(y.word) + \"'\\n\"\n",
        "                elif (y.tag.POS == \"INFN\"):\n",
        "                    strk = str(y.tag.POS) + \"[TR=\" + str(y.tag.transitivity) + \", TENSE=0, G=0, NUM=0, PER=0, NF=u'\" + str(y.normal_form) + \"'] -> '\" + str(y.word) + \"'\\n\"\n",
        "                else:\n",
        "                    strk = str(y.tag.POS) + \"[TR=\" + str(y.tag.transitivity) + \", TENSE=\" + str(y.tag.tense) + \", G=\" + \"0\" + \", NUM=\" + str(y.tag.number) + \", PER=\" + str(y.tag.person)[0] + \", NF=u'\" + str(y.normal_form) + \"'] -> '\" + str(y.word) + \"'\\n\"\n",
        "                f.writelines(strk)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "5RzTVC57xEF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"введите предложение для разбора: \") ## сюда пишется словосочетание для разбора\n",
        "words = word_tokenize(text.lower()) ## разбиваем словосочетание на токены\n",
        "\n",
        "pm2fcfg(words) ## запускаем функцию, описанную выше\n",
        "cp = load_parser('test.fcfg') ## открываем нашу грамматику, смотрим на разбор в консоли или ещё где\n",
        "for tree in cp.parse(words):\n",
        "\t\tprint(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "uLqjTVRGxIM2",
        "outputId": "62470e58-4025-4d30-a75c-792c9ca0e95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "введите предложение для разбора: двадцать третье марта\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-40b9679d8097>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpm2fcfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## запускаем функцию, описанную выше\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.fcfg'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## открываем нашу грамматику, смотрим на разбор в консоли или ещё где\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens, tree_class)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[0;34m(self, tokens, trace)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{w!r}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    666\u001b[0m                 \u001b[0;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[0;34m\"input words: %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'двадцать', 'третье', 'марта'\"."
          ]
        }
      ]
    }
  ]
}